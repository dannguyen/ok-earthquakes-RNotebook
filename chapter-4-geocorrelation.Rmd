---
title: "Chapter 4: Geolocation data"
author: "Dan Nguyen"
date: "August 17, 2015"
output:
  md_document:
    variant: markdown_github
---

Questions to answer:
- What are the geospatial characteristics of the recent Earthquakes?
- Is the recent spurt of earthquakes located in one area?
- What is the correlation between well activity and earthquakes?


## Setup

```{r, message = FALSE}
# Load libraries
library(ggplot2)
require(grid)
library(dplyr)
library(lubridate)
library(rgdal)
library(readxl)

# load my themes:
source("./myslimthemes.R")
theme_set(theme_dan())

# create a data directory
dir.create('./data')

```

Download the map data as before:

```{r, message = F}
fname <- "./data/cb_2014_us_state_20m.zip"
if (!file.exists(fname)){
  url <- "http://www2.census.gov/geo/tiger/GENZ2014/shp/cb_2014_us_state_20m.zip"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
unzip(fname, exdir = "./data/shp")

# Read the map data
us_map <- readOGR("./data/shp/cb_2014_us_state_20m.shp", "cb_2014_us_state_20m")
states_map <- us_map[!us_map$STUSPS %in%
                        c('AS', 'DC', 'GU', 'MP', 'PR', 'VI'),]

# For mapping purposes, we'll make a contiguous-states only
cg_map <- states_map[!states_map$STUSPS %in% c('AK', 'HI'), ]
# And for Oklahoma
ok_map <- states_map[states_map$STUSPS == 'OK', ]

```


Download the quakes data as before:


```{r, message = F}
fn <- './data/usgs-quakes-dump.csv'
zname <- paste(fn, 'zip', sep = '.')
if (!file.exists(zname) || file.size(zname) < 2048){
  url <- paste("https://github.com/dannguyen/ok-earthquakes-RNotebook",
    "raw/master/data", zname, sep = '/')
  print(paste("Downloading: ", url))
  # note: if you have problems downloading from https, you might need to include
  # RCurl
  download.file(url, zname, method = "libcurl")
}
unzip(zname, exdir="data")
# read the data into a dataframe
usgs_data <- read.csv(fn, stringsAsFactors = FALSE)

# Remove all non earthquakes and events with magnitude less than 3.0:
quakes <- usgs_data %>% filter(mag >= 3.0) %>%
  filter(type == 'earthquake')

quakes$year <- year(quakes$time)
quakes$year_month <- strftime(quakes$time, "%Y-%m")
quakes <- mutate(quakes, era = ifelse(year <= 2000, "1995-2000",
          ifelse(year <= 2005, "2001-2005",
          ifelse(year <= 2010, "2006-2010", "2011-2015"))))

# Create a spatial data frame----------------------------------
sp_quakes <- SpatialPointsDataFrame(data = quakes,
                          coords = quakes[,c("longitude", "latitude")])
sp_quakes@proj4string <- states_map@proj4string

# subset for earthquakes in the U.S.
xdf <- over(sp_quakes, states_map[, 'STUSPS'])
states_quakes <- cbind(sp_quakes, xdf) %>% filter(!is.na(STUSPS))

# And for convenience, OK-only
ok_quakes <- filter(states_quakes, STUSPS == 'OK')
```











## Injection volumes

Download # http://www.occeweb.com/og/ogdatafiles2.htm

2011:

```{r, message = F}
fname <- "./data/2011_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2011INJECTIONVOLUMES.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
tdf <- read_excel(fname)
# remove NA columns http://stackoverflow.com/questions/15968494/how-to-delete-columns-with-na-in-r
injections_data.2011 <- tdf[, colSums(is.na(tdf)) != nrow(tdf)] %>%
    mutate(Volume = as.integer(Volume))

sum(injections_data.2011$Volume)
```

2012:

```{r, message = F}
fname <- "./data/2012_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2012Injections20141029.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
tdf  <- read_excel(fname)
injections_data.2012 <- tdf[, colSums(is.na(tdf)) != nrow(tdf)] %>%
    mutate(Volume = as.numeric(Volume)) %>%
    filter(!is.na(Volume))
sum(injections_data.2012$Volume)
```


2013:

```{r, message = F}
fname <- "./data/2013_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2013%20UIC%20Injection%20Volumes%20Report.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
injections_data.2013 <- read_excel(fname)
tdf  <- read_excel(fname)
injections_data.2013 <- tdf[, colSums(is.na(tdf)) != nrow(tdf)] %>%
    mutate(Volume = as.numeric(Volume)) %>%
    filter(!is.na(Volume))
sum(injections_data.2013$Volume)

```




```{r}
injections.2013 <- mutate(injections_data.2013, API = as.character(API)) %>%
    filter(!is.na(X), X != 0,  X < -90, X > -100,
           !is.na(Y), Y != 0, Y > 33.5, Y < 40)

sum(injections.2013$Volume)
tk.2013 <- as.data.frame(group_by(injections.2013, API, X, Y, WellStatus) %>%
    summarise(counts = n(), total_volume = sum(Volume)) %>%
    arrange(desc(counts)))

#filter(uic_injects, API == '3.5103243e+13')

# TK count this
#length(unique(uic_injects$API))
#length(unique(uic_injects$WellNumber))



```






-------------
Map it:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  geom_point(data = filter(injections, total_volume < 1250000),
             aes(x = X, y = Y),
             alpha = 0.1 , color = 'yellow') +
  geom_point(data = filter(injections, total_volume > 1250000),
             aes(x = X, y = Y),
             alpha = 0.0 , color = 'green') +

  geom_point(data = ok_quakes, aes(x = longitude, y = latitude),
            alpha = 0.1, size = 0.2, shape = 1, color = 'red') +
  coord_map("albers", lat0 = 38, latl = 42) + theme_dan_map()
```



## Maps


Justify the use of Hexbin map



```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = injections, aes(x = X, y = Y, z = total_volume), alpha = 0.9, bins = 20 ) + geom_point(data = ok_quakes, aes(x = longitude, y = latitude),
           alpha = 0.1, size = 0.2, shape = 1, color = 'red')

```




-------------


```{r}

sp_ok_quakes <- SpatialPointsDataFrame(data = ok_quakes,
                          coords = ok_quakes[,c("longitude", "latitude")])
proj4string(sp_ok_quakes) <- proj4string(ok_map)

sp_injections <- SpatialPointsDataFrame(data = injections,
                          coords = injections[,c("X", "Y")])

proj4string(sp_injections) <- proj4string(ok_map)

albers_crs <- CRS("+proj=laea +lat_0=45 +lon_0=-100 +x_0=0 +y_0=0 +a=6370997 +b=6370997 +units=m +no_defs")
ok_map.albers <- spTransform(ok_map, albers_crs)
ok_quakes.albers <- as.data.frame(spTransform(sp_ok_quakes, albers_crs))
injections.albers <- as.data.frame(spTransform(sp_injections, albers_crs))

###0-----------
# look ma, no coord_map
ggplot() +
  geom_polygon(data = ok_map.albers, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  geom_point(data = filter(injections.albers, total_volume > 100000),
             aes(x = X, y = Y),
             alpha = 0.1 , color = 'yellow') +

  geom_point(data = ok_quakes.albers, aes(x = longitude, y = latitude),
            alpha = 0.1, size = 0.2, shape = 1, color = 'red')
```





<!--
To render this file:
library(rmarkdown)
setwd("~/Dropbox/rprojs/ok-earthquakes-Rnotebook/")

this_file <- 'chapter-4-geocorrelation.Rmd'
render(this_file, output_dir = './builds',
  html_document(toc = TRUE, self_contained = F))

render(this_file, output_dir = './builds',
  md_document(variant = "markdown_github",
              preserve_yaml = TRUE))
-->


The study, titled "Oklahoma's Recent Earthquakes and Saltwater Disposal," was funded by the Stanford Center for Induced and Triggered Seismicity, an affiliate program at the School of Earth, Energy & Environmental Sciences.


> Active faults in Oklahoma might trigger an earthquake every few thousand years. However, by increasing the fluid pressure through disposal of wastewater into the Arbuckle formation in the three areas of concentrated seismicity – from about 20 million barrels per year in 1997 to about 400 million barrels per year in 2013 – humans have sped up this process dramatically. "The earthquakes in Oklahoma would have happened eventually," Walsh said. "But by injecting water into the faults and pressurizing them, we've advanced the clock and made them occur today."



http://news.stanford.edu/news/2015/june/okla-quake-drilling-061815.html
