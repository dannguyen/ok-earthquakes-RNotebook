---
title: "Chapter 4: Correlations and Limitations"
author: "Dan Nguyen"
date: "August 17, 2015"
output:
  md_document:
    variant: markdown_github
---

<span id="chapter-4-mark"></span>

# Chapter 4: Correlation, limitations, and known unknowns


Questions to answer:
- What are the geospatial characteristics of the recent Earthquakes?
- Is the recent spurt of earthquakes located in one area?
- What is the correlation between well activity and earthquakes?


## Setup


- [readxl](https://github.com/hadley/readxl) - Later on we'll need to read from an XLS file. Again, via Hadley Wickham.



```{r, warning = F, message = FALSE}
# Load libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(dplyr)
library(lubridate)
library(rgdal)
library(readxl)
library(scales)
library(ggmap)
# load my themes:
source("./myslimthemes.R")
theme_set(theme_dan())

# create a data directory
dir.create('./data')

```

Download the map data as before:

```{r, warning = F, message = F, results = "hide"}
fname <- "./data/cb_2014_us_state_20m.zip"
if (!file.exists(fname)){
  url <- "http://www2.census.gov/geo/tiger/GENZ2014/shp/cb_2014_us_state_20m.zip"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
unzip(fname, exdir = "./data/shp")
#
# Read the map data
us_map <- readOGR("./data/shp/cb_2014_us_state_20m.shp", "cb_2014_us_state_20m")
states_map <- us_map[!us_map$STUSPS %in%
                        c('AS', 'DC', 'GU', 'MP', 'PR', 'VI'),]

```


Download and read the quakes data as before:


```{r, warning = F, message = F}
fn <- './data/usgs-quakes-dump.csv'
zname <- paste(fn, 'zip', sep = '.')
if (!file.exists(zname) || file.size(zname) < 2048){
  url <- paste("https://github.com/dannguyen/ok-earthquakes-RNotebook",
    "raw/master/data", zname, sep = '/')
  print(paste("Downloading: ", url))
  # note: if you have problems downloading from https, you might need to include
  # RCurl
  download.file(url, zname, method = "libcurl")
}
unzip(zname, exdir="data")
# read the data into a dataframe
usgs_data <- read.csv(fn, stringsAsFactors = FALSE)
```

Filter and mutate the data

```{r, warning = F, message = F}
# Remove all non earthquakes and events
# with magnitude less than 3.0:
quakes <- usgs_data %>%
          filter(mag >= 3.0, type == 'earthquake') %>%
          mutate(time = ymd_hms(time)) %>%
          mutate(date = as.Date(time)) %>%
          mutate(year = year(time)) %>%
          mutate(era = ifelse(year <= 2000, "1995-2000",
            ifelse(year <= 2005, "2001-2005",
            ifelse(year <= 2010, "2006-2010", "2011-2015"))))
```

```

```{r, warning = F, message = F}
sp_quakes <- SpatialPointsDataFrame(data = quakes,
                coords = quakes[,c("longitude", "latitude")])
sp_quakes@proj4string <- states_map@proj4string
# subset for earthquakes in the U.S.
xdf <- over(sp_quakes, states_map[, 'STUSPS'])
states_quakes <- cbind(sp_quakes, xdf) %>% filter(!is.na(STUSPS))
# add a convenience column for Oklahoma:
states_quakes <- mutate(states_quakes, is_OK = STUSPS == 'OK')
```

Making data frames for just Oklahoma:

```{r, warning = F, message = F}
# Dataframe of just Oklahoma quakes:
ok_quakes <- filter(states_quakes, is_OK)
# map of just Oklahoma state:
ok_map <- states_map[states_map$STUSPS == 'OK',]
```






## NEW Injection volumes

Download # http://www.occeweb.com/og/ogdatafiles2.htm

### The 2013 data

```{r, warning = F, message = F}
# Download the data
fname <- "./data/2013_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2013%20UIC%20Injection%20Volumes%20Report.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
```

Read the data

```{r, warning = F, message = F}
xl_2013 <- read_excel(fname)
```


We're going to get a ton of warnings that look like this:

      1: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
        [1831, 11]: expecting numeric: got 'NULL'


```{r, warning = F, message = F}
injections_2013 <-  xl_2013 %>%
    mutate(Volume = as.numeric(Volume),
           API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(ISDELETE == 0, SALTWATER == 1, STATUS == 'ACCEPTED',
           !is.na(Volume), !is.na(X))
```


Aggregate the wells by their unique identifier and create a `annual_volume` column:


```{r, warning = F, message = F}
wells_2013 <-  injections_2013 %>%
    group_by(ReportYear, API, X, Y) %>%
    summarise(count = n(), annual_volume = sum(Volume))
```

Some wells have 24 records. It appears to be a double count:


```{r, warning = F, message = F}
filter(injections_2013, API == '35003220260000', Month == 'MAY') %>% select(Month, Volume)
```

Sigh.

Let's just average things.


```{r, warning = F, message = F}
clean_wells_2013 <-  bind_rows(
    filter(wells_2013, count <= 12),
    filter(wells_2013, count > 12) %>%
      mutate(annual_volume = annual_volume / (count / 12)))
```

How close are we to Walsh's estimate of 160 million barrels a month?


```{r}
100 * (160000000 - (sum(clean_wells_2013$annual_volume) / 12)) / 160000000
```


About 10 percent off. Well, at least we're in the same ballpark...This discrepancy seems reasonable given that I've done about zero data cleaning. 

#### Spatial filtering

But there is one more thing we can do: There are typos in the `X` and `Y` columns, i.e. the wells aren't properly geolocated. We could hand-inspect each instance and try to manually fix the data ourselves. But nah, let's just use `ok_map` to filter out all wells that aren't within the Oklahoma state boundaries:


```{r, warning = F, message = F}
tmpdf <- as.data.frame(clean_wells_2013)
sp_wells_2013 <-  SpatialPointsDataFrame(data = tmpdf,
                          coords = tmpdf[, c("X", "Y")])
sp_wells_2013@proj4string <- ok_map@proj4string
# subset for wells in Oklahoma
mapped_wells_2013 <- as.data.frame(sp_wells_2013[ok_map, ]) 
```

_Now_ let's calculate the average injection volume per month and find how much we're off compared to Walsh's estimation of 160 million barrels a month:

```{r}
100 * (160000000 - (sum(mapped_wells_2013$annual_volume) / 12)) / 160000000
```

Only 1.6% off. Not bad for doing the most blunt and shallow kind of data cleaning.

But how different is the volume of the mappable wells compared to the non-mappable wells in our dataset?


```{r}
100 * (sum(mapped_wells_2013$annual_volume) - 
         sum(clean_wells_2013$annual_volume)) / 
  sum(clean_wells_2013$annual_volume)
```

11%. It's a little disconcerting that as much as 10% of relevant data is being ignored because their coordinates are off -- and that's only scratching the surface of possible data entry problems. But, I think we can move on. Again, as I cannot stress enough: this is only a walkthrough, not an actual peer-reviewed research paper.

### 2012 UIC data




```{r, warning = F, message = F}
fname <- "./data/2012_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2012Injections20141029.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2012  <- read_excel(fname)
```

Among the new problems 2012 has:

```{r, warning = F, message = F}
injections_2012 <- xl_2012 %>%
    mutate(Volume = as.numeric(Volume),
          API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(ISDELETE == 0, SALTWATER == 1, STATUS == 'ACCEPTED',
           !is.na(Volume))
```


Let's do an aggregation and get the `annual_volume`. Then derive `clean_wells` by compensating for wells in which the count of records is more than 12:


```{r, warning = F, message = F}
wells_2012 <-  injections_2012 %>%
    group_by(ReportYear, API) %>%
    summarise(count = n(), annual_volume = sum(Volume))
```


```{r, warning = F, message = F}
clean_wells_2012 <-  bind_rows(
    filter(wells_2012, count <= 12),
    filter(wells_2012, count > 12) %>%
      mutate(annual_volume = annual_volume / (count / 12)))
```


#### Left join for latitude/longitudes

So the big problem with the 2012 data is that there is no `X` or `Y` column, i.e. the geospatial data.

But we can fix that by joining `clean_wells_2012` to `mapped_wells_2013`, the latter of which _does_ contain geospatial data. Both tables contain an `API` column. But let me be clear: it is a _major_ assumption to think that the API values, whatever they stand for, aren't also filled with typos or nonsense data.

For SQL fans, the dplyr package has a conveniently named __left_join()__ function. If you're fuzzy on how left join's work, remember that it preserves all the records on the "left" side -- in this case, `clean_wells_2012`. So after doing the join, we perform a filter to remove rows in which `X` (or `Y`) is NA:


```{r}
mapped_wells_2012 <- clean_wells_2012 %>%
  left_join(select(as.data.frame(mapped_wells_2013), API, X, Y), by = 'API') %>%
  filter(!is.na(X))
```

Let's calculate the monthly average for 2012:

```{r}
sum(mapped_wells_2012$annual_volume) / 12 
```

And how much did we lose by removing wells in 2012 that didn't have a matching API in 2013?


```{r}
100 * (sum(mapped_wells_2012$annual_volume) - 
         sum(clean_wells_2012$annual_volume)) / 
  sum(clean_wells_2012$annual_volume)
```

That's more than a rounding error within the 2012 data...but moving on to 2011.

### 2011 data

And the data keeps getting worse:

```{r, warning = F, message = F}
fname <- "./data/2011_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2011INJECTIONVOLUMES.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2011 <- read_excel(fname)
```

To see how ugly things are, check out the column names:

```{r}
names(xl_2011)
```

Columns 27 to 49 exist, but have null values. Usually this is the result of some sloppy mainframe-to-spreadsheet program that someone wrote decades ago. But now we have to deal with it because R does not like it when a data frame has duplicate values in the column names.

This turns out not to be too horrible, using standard R syntax to subset the selection by choosing only the first 27 columns. Then we apply the same mutations and filters as we did for the 2012 data set. We have to make a few changes because whoever maintains the 2011 dataset decided to use a whole different naming scheme, e.g. `IsDelete` instead of `ISDELETE` _and_ datatype convention, e.g. `"0"` instead of `0`:

```{r}
injections_2011 <- xl_2011[, c(1:26)] %>%
    mutate(Volume = as.numeric(Volume),
          API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(IsDelete == "0", SaltWater == "1", Status == 'ACCEPTED',
           !is.na(Volume))
```

Like 2012, there are no `X` and `Y`  columns. So we repeat 2012's filtering and joining process: 

```{r, warning = F, message = F}
# sum the wells
wells_2011 <-  injections_2011 %>%
    group_by(ReportYear, API) %>%
    summarise(count = n(), annual_volume = sum(Volume))
# clean the wells
clean_wells_2011 <-  bind_rows(
    filter(wells_2011, count <= 12),
    filter(wells_2011, count > 12) %>%
      mutate(annual_volume = annual_volume / (count / 12)))
# geo-join the wells
mapped_wells_2011 <- clean_wells_2011 %>%
  left_join(select(as.data.frame(mapped_wells_2013), API, X, Y), by = 'API') %>%
  filter(!is.na(X))
```

Let's calculate the percent difference between the mappable and non-mappable wells:

```{r}
100 * (sum(mapped_wells_2011$annual_volume) - 
         sum(clean_wells_2011$annual_volume)) / 
  sum(clean_wells_2011$annual_volume)
```

It's roughly the same difference as found in 2012. Meh, moving on.


### 2006 to 2010 UIC data

The next data file combines the years 2006 to 2010 into a single spreadsheet. That's a strong cue that the data structure will likely be signficantly different than what we have so far:

```{r, warning = F, message = F}
fname <- "./data/all_2006-2010uic_1012a.xls"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/all_2006-2010uic_1012a.xls"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2006_2010 <- read_excel(fname)
```

You can run `names(xl_2006_2010)` yourself, but the upshot is that this data file is much simpler than what we have so far with only 10 columns. Each row represents an annual measurement of volume, which saves us the time of aggregating it ourselves. 

Unlike the other datasets, there is a `FLUID_TYPE` column which very _unhelpfully_ contains values such as `"S"` and `""`:


```{r}
group_by(xl_2006_2010, FLUID_TYPE) %>% summarize(count = n())
```

Since saltwater injections make up the vast majority of UIC wells, I'm just going to assume that `S` stands for "saltwater". As a bonus nusiance, the column name for `"TOTAL VOLUME"` is a bit jacked up with extra whitespace. So I'm just going to manually rename the columns as part of the initial data filtering step:

```{r}
# rename the columns
names(xl_2006_2010) <- c('API_COUNTY', 'API', 'LEASE_NAME', 'WELL_NUMBER', 'Y', 'X', 'ReportYear', 'FLUID_TYPE', 'PACKERDPTH', 'annual_volume')
# filter the data
clean_wells_2006_2010 <- xl_2006_2010 %>%
    mutate(annual_volume = as.numeric(annual_volume),
           API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(!is.na(annual_volume), !is.na(X)) %>%
    select(API, X, Y, ReportYear, annual_volume)
```

Since this data is already aggregated by year, we can skip the summarization step. And since it comes with `X` and `Y`, we don't need to do a table join with `mapped_wells_2013`. But we do need to repeat the spatial filtering with `ok_map`:


```{r, warning = F, message = F}
tmpdf <- as.data.frame(clean_wells_2006_2010)
sp_wells_2006_2010 <-  SpatialPointsDataFrame(data = tmpdf,
                          coords = tmpdf[, c("X", "Y")])
sp_wells_2006_2010@proj4string <- ok_map@proj4string
# subset for wells in Oklahoma
mapped_wells_2006_2010 <- as.data.frame(sp_wells_2006_2010[ok_map, ]) 
```

Let's calculate the difference between mapped and non-mappable wells:

```{r}
100 * (sum(mapped_wells_2006_2010$annual_volume) - 
         sum(clean_wells_2006_2010$annual_volume)) / 
  sum(clean_wells_2011$annual_volume)
```

Less than 3 hundredths of a percent, not bad! Though that doesn't help us with the discrepencies in years 2011, 2012, and 2013...


### All the UIC data together now

We can now create a data frame of annual saltwater injection volumes for 2006 to 2013. All that grunt work makes this collation quite easy. I use dplyr's __bind_rows()__ function, which I believe is just a wrapper around __rbind()__. Some of the wells have typos in the `ReportYear`, so we apply a filter to remove all years greater than 2013:

```{r}
all_mapped_wells <- bind_rows(mapped_wells_2013, 
                       mapped_wells_2012, 
                       mapped_wells_2011, 
                       mapped_wells_2006_2010) %>%
                    filter(ReportYear <= 2013)
```


Now let's make a histogram:


```{r}
ggplot(data = group_by(all_mapped_wells, ReportYear)
                          %>% summarize(total_volume = sum(annual_volume)),
        aes(x = ReportYear, y = total_volume)) + 
  geom_bar(stat = 'identity') + 
  ggtitle("Mappable wells")
```

Let's look at the difference between mappable and cleaned:

```{r}
all_cleaned_wells <- bind_rows(clean_wells_2013, 
                       clean_wells_2012, 
                       clean_wells_2011, 
                       clean_wells_2006_2010) %>%
                    filter(ReportYear <= 2013)
```


Compare the two:


```{r}
ggplot(data = group_by(all_cleaned_wells, ReportYear)
                          %>% summarize(total_volume = sum(annual_volume)),
        aes(x = ReportYear, y = as.integer(total_volume / 1000000))) + 
  geom_bar(stat = 'identity', fill = "red") + 
  geom_bar(stat = 'identity',
           data = group_by(all_mapped_wells, ReportYear)
                          %>% summarize(total_volume = sum(annual_volume)),
           fill = "black"
           ) + 
  scale_y_continuous(labels = comma, expand = c(0, 0)) +
  ggtitle("Mappable vs cleaned wells")
```




Summarise by location


```{r}
q1 <- ggplot(data = all_wells %>%
         group_by(Y, ReportYear) %>% summarize(total = sum(total_volume)),
       aes(x = factor(ReportYear), y = total, order = desc(Y >= 36),
           fill = Y >= 36)) + geom_bar(stat = 'identity') +
        scale_fill_manual(values = c("#EEEEEE", "#FF6600"), labels = c("Below 36.5", "Above 36.5")) +
        scale_x_discrete(limits = c(2006:2015), labels = c("2012", "2013") )+
      guides(fill = guide_legend(reverse = F))
```


Histogram of quakes

```{r}
q2 <- ggplot(data = filter(ok_quakes, year >= 2006),
       aes(x = factor(year), order = desc(latitude >= 36),
           fill = latitude >= 36)) + geom_histogram(binwidth = 1) +
        scale_fill_manual(values = c("#EEEEEE", "#FF6600"), labels = c("Below 36.5", "Above 36.5")) +
      guides(fill = guide_legend(reverse = F))
```


```{r}
grid.arrange(
  q1 + theme(axis.text.y = element_blank()) + ggtitle("Annual injection volumes"),
  q2 + theme(axis.text.y = element_blank()) + ggtitle("Annual count of M3.0+ earthquakes")
)
```



Facet wrap the wells:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  geom_point(data = filter(all_wells, total_volume <= 360000),
             aes(x = X, y = Y),
             alpha = 0.3, shape = 1, color = 'yellow') +
  geom_point(data = filter(all_wells, total_volume >= 360000),
             aes(x = X, y = Y),
             alpha = 0.3, shape = 1, color = 'purple') +
  # geom_point(data = filter(ok_quakes, year >= 2012), aes(x = longitude, y = latitude),
  #           alpha = 0.1, shape = 1, color = 'red') +
  coord_map("albers", lat0 = 38, latl = 42) + theme_dan_map() +
  ggtitle("2013 UIC and quakes")  +
  facet_wrap(~ ReportYear, ncol = 2)
```

TODO: Hexbin them


Counts:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +

  stat_binhex(data = all_wells, aes(x = X, y = Y, alpha = ..count..), bins = 30 ) +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```


Volume



```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = all_wells,
                   aes(x = X, y = Y, z = total_volume),
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 1)
```



Hi-lo pass:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = all_wells,
                   aes(x = X, y = Y, z = total_volume),
                   bins = 30,
                   fun = function(z){
                     sz <- min(sum(z), 60000000)
                     ifelse(sz > 10000000, sz, NA)
                    },
                   color = 'white', drop = TRUE ) +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```





Do side by side facet of earthquakes


```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_binhex(data = filter(ok_quakes, year >= 2012, year <= 2014),
              aes(x = longitude, y = latitude), bins = 30, color = "#FFFFFF") +
  scale_fill_gradient(low = "#ECDDDD", high = "red") +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 1)
```



#### Draw a highlight box

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear >= 2010),
                   aes(x = X, y = Y, z = total_volume),
                   bins = 30,
                   fun = function(z){
                     sz <- min(sum(z), 60000000)
                     ifelse(sz > 10000000, sz, NA)
                    },
                   color = 'white', drop = TRUE ) +
  geom_rect(data = ok_map, aes(
      xmin = -99, xmax = -97.5,
      ymin = 36, ymax = 37
    ), fill = "NA", color = "red", linetype = "dashed", size = 0.5) +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```

Do the same with earthquakes:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_binhex(data = filter(ok_quakes, year >= 2011, year <= 2014),
              aes(x = longitude, y = latitude), bins = 30, color = "#FFFFFF") +
  geom_rect(data = ok_map, aes(
      xmin = -99, xmax = -97.5,
      ymin = 36, ymax = 37
    ), fill = "NA", color = "red", linetype = "dashed", size = 0.5) +

  scale_fill_gradient(low = "#ECDDDD", high = "red") +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 2)
```


### Test, volume and alpha:

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear == 2006),
                   aes(x = X, y = Y, z = total_volume),
                   alpha = 0.5,
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "#002BFF"), na.value = NA) +
    coord_equal() +
  theme_dan_map()
```

```{r}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear == 2013),
                   aes(x = X, y = Y, z = total_volume),
                   alpha = 0.5,
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "#FFD500"), na.value = NA) +
    coord_equal() +
  theme_dan_map()
```



# Gas prices

https://research.stlouisfed.org/fred2/series/DCOILWTICO/downloaddata


```{r, warning = F, message = F}
fname <- './data/stlouisfed-oil-prices.csv'
if (!file.exists(fname)){
  curlstr <- paste("curl -s --data 'form%5Bnative_frequency%5D=Daily&form%5Bunits%5D=lin&form%5Bfrequency%5D=Daily&form%5Baggregation%5D=Average&form%5Bobs_start_date%5D=1986-01-02&form%5Bobs_end_date%5D=2015-08-24&form%5Bfile_format%5D=csv&form%5Bdownload_data_2%5D='", '-o', fname)
  print(paste("Downloading: ", fname))
  system(curlstr)
}
gasdata <- read.csv("./data/stlouisfed-oil-prices.csv", stringsAsFactors = F) %>%
  filter(year(DATE) >= 2008, VALUE != '.')  %>%
  mutate(week = floor_date(as.Date(DATE), 'week'), VALUE = as.numeric(VALUE), panel = "GAS") %>%
  group_by(week) %>% summarize(avg_value = mean(VALUE))


```







```{r, warning = F, message = F}
okquakes_weekly <- mutate(ok_quakes, week = floor_date(date, 'week'), panel = 'Quakes') %>%
  filter(year >= 2008) %>%
  group_by(week) %>%
  summarize(count = n())


gp <- ggplot() + scale_x_date(breaks = pretty_breaks(),
                              limits = c(as.Date('2008-01-01'), as.Date('2015-08-31')))

quakesg <- gp + geom_bar(data = okquakes_weekly, aes(x = week, y = count), stat = "identity", position = 'identity', color = 'black') +
  scale_y_continuous(expand = c(0, 0)) +
  ggtitle("Weekly Counts of M3.0+ Earthquakes in Oklahoma")

gasg <- gp + geom_line(data = gasdata, aes(x = week, y = avg_value), size = 0.5, color = 'black') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 150)) +
  ggtitle("Weekly Average Oil Prices, Dollars per Barrel")
```


```{r, warning = F, message = F}
# http://stackoverflow.com/questions/10197738/add-a-footnote-citation-outside-of-plot-area-in-r

annotations <- list(
  list(date = "2011-11-05", label = "5.6M earthquake hits Oklahoma", letter = 'A'),
  list(date = "2014-02-18", label = 'OGS states that "Overall, the majority, but not all, of the recent earthquakes appear\nto be the result of natural stresses."', letter = 'B'),
  list(date = "2015-04-21", label = 'OGS reverses its opinion: "The rates and trends in seismicity are very unlikely\nto represent a naturally occurring process"', letter = 'C')
)


datelines <- lapply(annotations, function(a){
  wk <- as.numeric(floor_date(as.Date(a$date), 'week'))
  geom_vline(xintercept = wk, color = "red",
             linetype = 'dashed')
})

datelabels <- lapply(annotations, function(a){
  dt <- as.Date(a$date)
  annotate("text", x = dt, y = Inf, size = rel(4.0),  label = a$letter, vjust = 1.0, fontface = 'bold', color = "red", hjust = 1.5)
})

ttheme <- theme(plot.title = element_text(size = 12))

footnotes = paste(lapply(annotations, function(a){
              paste(a$letter,
              paste(strftime(a$date, '%b %d, %Y'), a$label, sep = ' - '),
              sep = '. ')
    }),
  collapse = '\n')

p <- arrangeGrob(quakesg + datelines + datelabels + ttheme,
                 gasg + datelines + datelabels + ttheme,
                 bottom = textGrob(label = footnotes, x = unit(1, 'cm'),
                                         hjust = 0.0, vjust = 0.3,
                                         gp = gpar(fontsize = rel(10), fontfamily = "Gill Sans MT"))
                 )

plot.new()
grid.draw(p)
```




The study, titled "Oklahoma's Recent Earthquakes and Saltwater Disposal," was funded by the Stanford Center for Induced and Triggered Seismicity, an affiliate program at the School of Earth, Energy & Environmental Sciences.


> Active faults in Oklahoma might trigger an earthquake every few thousand years. However, by increasing the fluid pressure through disposal of wastewater into the Arbuckle formation in the three areas of concentrated seismicity – from about 20 million barrels per year in 1997 to about 400 million barrels per year in 2013 – humans have sped up this process dramatically. "The earthquakes in Oklahoma would have happened eventually," Walsh said. "But by injecting water into the faults and pressurizing them, we've advanced the clock and made them occur today."



http://news.stanford.edu/news/2015/june/okla-quake-drilling-061815.html






## Trying Google Maps


```{r, warning = F, message = F}
library(ggmap)
ok_goog_map <- get_googlemap(center = c(lon = -99.007, lat = 35.38905),  size = c(450,250), zoom = 6, scale = 2, maptype = 'terrain',
style = c(feature = "administrative.province", element = "labels", visibility = "off"))
ggmap(ok_goog_map)
```

Stamen map

```{r, warning = F, message = F}
ok_bounds <- c( left = -103.3,
                bottom = 33.5,
                top = 37.0,
                right = -94.2
                )
ok_stamen_map <- get_stamenmap(bbox = ok_bounds, zoom = 7, maptype =  "toner-hybrid",crop = T)

ggmap(ok_stamen_map)
```


------




## Things TODO

- Use USGS Geologic map data: https://mrdata.usgs.gov/geology/state/state.php?state=OK



Reference to disposal wells in OK gov:

http://earthquakes.ok.gov/what-we-know/earthquake-map/


On April 21, earthquakes.ok.gov was launched:

http://earthquakes.ok.gov/news/



## The state of research

[NPR's Oklahoma StateImpact project has a nice reading list](https://stateimpact.npr.org/oklahoma/2015/05/05/stateimpacts-earthquake-research-reading-list/), and so does the state of Oklahoma on [earthquakes.ok.gov](http://earthquakes.ok.gov/what-we-know/academic-research/). Much of what I include below is cribbed from those collections:


-  [Texas Railroad Commission Refutes Study Linking Quakes to Oil and Gas Industry](https://stateimpact.npr.org/texas/2015/09/02/texas-railroad-commission-refutes-study-linking-quakes-to-oil-and-gas-industry/)



http://www.newyorker.com/magazine/2015/04/13/weather-underground


> Holland had been clear about the connections between disposal wells and earthquakes, and during the socializing a researcher from Princeton observed that Holland’s position seemed to have shifted from that represented in O.G.S. statements. “Let me think how I can answer that while there’s a reporter standing right there,” Holland said, lightly. “The O.G.S. is a nonacademic department of a state university that, like many state universities, doesn’t get that much funding from the state.” The O.G.S. is part of O.U.’s Mewbourne College of Earth and Energy, which also includes the ConocoPhillips School of Geology and Geophysics. About seventeen per cent of O.U.’s budget comes from the state. “I prepare twenty pages for those statements and what comes out is one page. Those are not necessarily my words.”


In Science Magazine ("[High-rate injection is associated with the increase in U.S. mid-continent seismicity](https://profile.usgs.gov/myscience/upload_folder/ci2015Jun1814143055600Weingarten_etal.pdf)"), USGS scientists assert that "the entire increase in earthquake rate is associated with fluid injection wells."


[High-rate injection is associated with the increase in U.S. mid-continent seismicity
](http://www.sciencemag.org/content/348/6241/1336)

> Wastewater injection wells induce earthquakes that garner much attention, especially in tectonically inactive regions. Weingarten et al. combined information from public injection-well databases from the eastern and central United States with the best earthquake catalog available over the past 30 years. The rate of fluid injection into a well appeared to be the most likely decisive triggering factor in regions prone to induced earthquakes. Along these lines, Walsh III and Zoback found a clear correlation between areas in Oklahoma where waste saltwater is being injected on a large scale and areas experiencing increased earthquake activity.


[Oklahoma’s recent earthquakes and saltwater disposal](http://advances.sciencemag.org/content/1/5/e1500195.article-info) - F. Rall Walsh III and Mark D. Zoback

> Over the past 5 years, parts of Oklahoma have experienced marked increases in the number of small- to moderate-sized earthquakes. In three study areas that encompass the vast majority of the recent seismicity, we show that the increases in seismicity follow 5- to 10-fold increases in the rates of saltwater disposal. Adjacent areas where there has been relatively little saltwater disposal have had comparatively few recent earthquakes. In the areas of seismic activity, the saltwater disposal principally comes from “produced” water, saline pore water that is coproduced with oil and then injected into deeper sedimentary formations. These formations appear to be in hydraulic communication with potentially active faults in crystalline basement, where nearly all the earthquakes are occurring. Although most of the recent earthquakes have posed little danger to the public, the possibility of triggering damaging earthquakes on potentially active basement faults cannot be discounted.

TK img: http://d3a5ak6v9sb99l.cloudfront.net/content/advances/1/5/e1500195/F1.large.jpg?download=true


TK img: http://d3a5ak6v9sb99l.cloudfront.net/content/advances/1/5/e1500195/F2.large.jpg?width=800&height=600&carousel=1


- [Sharp increase in central Oklahoma seismicity since 2008 induced by massive wastewater injection](http://www.sciencemag.org/content/345/6195/448)



> Unconventional oil and gas production provides a rapidly growing energy source; however, high-production states in the United States, such as Oklahoma, face sharply rising numbers of earthquakes. Subsurface pressure data required to unequivocally link earthquakes to wastewater injection are rarely accessible. Here we use seismicity and hydrogeological models to show that fluid migration from high-rate disposal wells in Oklahoma is potentially responsible for the largest swarm. Earthquake hypocenters occur within disposal formations and upper basement, between 2- and 5-kilometer depth. The modeled fluid pressure perturbation propagates throughout the same depth range and tracks earthquakes to distances of 35 kilometers, with a triggering threshold of ~0.07 megapascals. Although thousands of disposal wells operate aseismically, four of the highest-rate wells are capable of inducing 20% of 2008 to 2013 central U.S. seismicity.


- [Myths and Facts on Wastewater Injection, Hydraulic Fracturing, Enhanced Oil Recovery, and Induced Seismicity](http://srl.geoscienceworld.org/content/86/4/1060.full)


- [Observations of static Coulomb stress triggering of the November 2011 M5.7 Oklahoma earthquake sequence](http://onlinelibrary.wiley.com/doi/10.1002/2013JB010612/abstract)


TKTK:
http://energyindepth.org/national/five-things-to-know-about-a-new-stanford-oklahoma-earthquake-study/