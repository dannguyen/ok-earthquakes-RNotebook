---
title: "Chapter 4: Correlations and Limitations"
author: "Dan Nguyen"
date: "August 17, 2015"
output:
  md_document:
    variant: markdown_github
---

<a id="mark-chapter-4"></a>

# Chapter 4: Correlation, limitations, and known unknowns


Questions to answer:
- What are the geospatial characteristics of the recent Earthquakes?
- Is the recent spurt of earthquakes located in one area?
- What is the correlation between well activity and earthquakes?


## Setup


- [readxl](https://github.com/hadley/readxl) - Later on we'll need to read from an XLS file. Again, via Hadley Wickham.



```{r, warning = F, message = FALSE}
# Load libraries
library(ggplot2)
library(grid)
library(gridExtra)
library(dplyr)
library(lubridate)
library(rgdal)
library(readxl)
library(scales)
library(ggmap)
# load my themes:
source("./myslimthemes.R")
theme_set(theme_dan())

# create a data directory
dir.create('./data')

```

Download the map data as before:

```{r, warning = F, message = F, cache = TRUE, results = "hide", cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
fname <- "./data/cb_2014_us_state_20m.zip"
if (!file.exists(fname)){
  url <- "http://www2.census.gov/geo/tiger/GENZ2014/shp/cb_2014_us_state_20m.zip"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
unzip(fname, exdir = "./data/shp")
#
# Read the map data
us_map <- readOGR("./data/shp/cb_2014_us_state_20m.shp", "cb_2014_us_state_20m")
states_map <- us_map[!us_map$STUSPS %in%
                        c('AS', 'DC', 'GU', 'MP', 'PR', 'VI'),]

```


Download and read the quakes data as before:


```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
fn <- './data/usgs-quakes-dump.csv'
zname <- paste(fn, 'zip', sep = '.')
if (!file.exists(zname) || file.size(zname) < 2048){
  url <- paste("https://github.com/dannguyen/ok-earthquakes-RNotebook",
    "raw/master/data", zname, sep = '/')
  print(paste("Downloading: ", url))
  # note: if you have problems downloading from https, you might need to include
  # RCurl
  download.file(url, zname, method = "libcurl")
}
unzip(zname, exdir="data")
# read the data into a dataframe
usgs_data <- read.csv(fn, stringsAsFactors = FALSE)
```

Filter and mutate the data

```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
# Remove all non earthquakes and events with magnitude less than 3.0:
quakes <- usgs_data %>%
          filter(mag >= 3.0, type == 'earthquake') %>%
          mutate(time = ymd_hms(time)) %>%
          mutate(date = as.Date(time)) %>%
          mutate(year = year(time))
```


```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
sp_quakes <- SpatialPointsDataFrame(data = quakes,
                coords = quakes[,c("longitude", "latitude")])
sp_quakes@proj4string <- states_map@proj4string
# subset for earthquakes in the U.S.
xdf <- over(sp_quakes, states_map[, 'STUSPS'])
states_quakes <- cbind(sp_quakes, xdf) %>% filter(!is.na(STUSPS))
# add a convenience column for Oklahoma:
states_quakes <- mutate(states_quakes, is_OK = STUSPS == 'OK')
```

Making data frames for just Oklahoma:

```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
# Dataframe of just Oklahoma quakes:
ok_quakes <- filter(states_quakes, is_OK)
# map of just Oklahoma state:
ok_map <- states_map[states_map$STUSPS == 'OK',]
```


## Location is everything

In [chapter 3](#mark-small_multiples_oklahoma_map_quakes), we created a few national and Oklahoma maps before moving on to histograms, which were more effective for quantifying the frequency and number of earthquakes.

But now that we're pretty well convinced that Oklahoma is facing unprecedented seismic activity, it's necessary to focus on the earthquakes' _locations_ in order to make any kind of correlation between their occurrence and human activity.

Revisiting this small-multiples map from [Chapter 3](#mark-small_multiples_oklahoma_map_quakes):

![mark-small_multiples_oklahoma_map_quakes](./images/mark-small_multiples_oklahoma_map_quakes.png)

Let's facet earthquakes by year, within the period of 2011 to 2015:

```{r, small_multiples_oklahoma_map_quakes_2011_2015,  message = F}
ok_quakes_2010_2015 <- filter(ok_quakes, year >= 2010)
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group), fill = "white", color = "#777777") +
  geom_point(data = ok_quakes_2010_2015, aes(x = longitude, y = latitude), shape = 1,  alpha = 0.5, color = "red") +
  coord_map("albers", lat0 = 38, latl = 42) +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 3) +
  ggtitle("Oklahoma M3.0+ earthquakes by time period")
```

### Getting geographically oriented

The Census boundaries have worked nicely for our charting. But they lack geographical information, such as city and landmark location. Let's use the [__ggmap__ library](https://github.com/dkahle/ggmap) to bring in OpenStreetMap data, via the [Stamen toner layers](http://maps.stamen.com/toner/#12/37.7706/-122.3782):


```{r, ok_stamen_map_base, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
ok_bbox <- c( left = -103.3, right = -94.2, bottom = 33.5, top = 37.1)
ok_stamen_map <- get_stamenmap(bbox = ok_bbox, zoom = 7, maptype =  "toner", crop = T)

gg_ok_map <- ggmap(ok_stamen_map) +
             geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
                           fill = NA, color = "#884400")

gg_ok_map
```

Rather than overlaying the earthquakes as dots, let's use hexbinning so that we don't obscure the geographical labels:

```{r, ok_stamen_map_2011_2015_hexbin, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4", fig.height = 12, fig.width = 12}
gg_ok_map +
  stat_binhex(data = filter(ok_quakes_2010_2015, year == 2011 | year == 2015), aes(x = longitude, y = latitude, color = ..count..), bins = 40, fill = NA) +
  guides(fill = F) +
  scale_color_gradientn(colours=c("yellow","red")) +
  facet_wrap(~ year, ncol = 1) +
  ggtitle("Oklahoma M3.0+ earthquake density, 2011 and 2015") +
  theme_dan_map()
```

Let's define two areas of interest:

Enid area:

```{r, enid_bbox, warning = F, message = F}
enid_bbox <- c(xmin = -99.2, xmax = -96.5, ymin = 36.2, ymax = 37)
enid_rect_layer <- annotate(geom = "rect", xmin = enid_bbox['xmin'],
                      xmax = enid_bbox['xmax'],
                      ymin = enid_bbox['ymin'],
                      ymax = enid_bbox['ymax'],
                      fill = "NA", color = "purple", linetype = "dashed", size = 1)
gg_ok_map + enid_rect_layer + ggtitle("Enid area")
```

OK City area:

```{r, okcity_bbox, warning = F, message = F}
okcity_bbox <- c(xmin = -98, xmax = -96.3, ymin = 35.3, ymax = 36.19)
okcity_rect_layer <- annotate(geom = "rect", xmin = okcity_bbox['xmin'],
                      xmax = okcity_bbox['xmax'],
                      ymin = okcity_bbox['ymin'],
                      ymax = okcity_bbox['ymax'],
                      fill = "NA", color = "forestgreen", linetype = "dashed", size = 1)
gg_ok_map + okcity_rect_layer + ggtitle("OK City area")
```


Let's examine our bounding boxes:

```{r, small_multiples_oklahoma_map_quakes_2011_2015_with_bboxes,  message = F}
ok_quakes_2010_2015 <- filter(ok_quakes, year >= 2010)
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group), fill = "white", color = "#777777") +
  okcity_rect_layer +
  enid_rect_layer +
  geom_point(data = ok_quakes_2010_2015, aes(x = longitude, y = latitude), shape = 1,  alpha = 0.5, color = "red") +
  coord_map("albers", lat0 = 38, latl = 42) +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 3) +
  ggtitle("Oklahoma M3.0+ earthquakes by time period")
```


Let's get the earthquakes per area:

```{r, warning = F, message = F}
# Oklahoma City area quakes
ok_city_quakes <- ok_quakes %>%
                      filter(longitude >= okcity_bbox['xmin'],
                             longitude < okcity_bbox['xmax'],
                             latitude >= okcity_bbox['ymin'],
                             latitude < okcity_bbox['ymax'])

enid_quakes <- ok_quakes %>%
                      filter(longitude >= enid_bbox['xmin'],
                             longitude < enid_bbox['xmax'],
                             latitude >= enid_bbox['ymin'],
                             latitude < enid_bbox['ymax'])
```

Our hypothesis: if injection wells are the cause of Oklahoma's recent earthquake surge, then we should expect to find increased injection well activity in the "Enid area" and "OK City area" bounding boxes.




## Oklahoma injection well data

Download # http://www.occeweb.com/og/ogdatafiles2.htm

### The 2013 data

```{r, warning = F, message = F}
# Download the data
fname <- "./data/2013_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2013%20UIC%20Injection%20Volumes%20Report.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
```

Read the data

```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
xl_2013 <- read_excel(fname)
```


We're going to get a ton of warnings that look like this:

      1: In read_xlsx_(path, sheet, col_names = col_names, col_types = col_types,  ... :
        [1831, 11]: expecting numeric: got 'NULL'


```{r, warning = F, message = F}
injections_2013 <-  xl_2013 %>%
    mutate(Volume = as.numeric(Volume),
           API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(ISDELETE == 0, SALTWATER == 1, STATUS == 'ACCEPTED',
           !is.na(Volume), !is.na(X))
```


Aggregate the wells by their unique identifier (`API`) and spatial coordinates and create a `raw_annual_volume` column. I also add a `count` column which, _theoretically_, should always be 12 or less, i.e. 12 _months_, assuming that each well puts out, at most, one monthly record for its injection activity. But that assumes a lot of things, such as no errors in the data entry process:


```{r, warning = F, message = F}
wells_2013 <-  injections_2013 %>%
    group_by(ReportYear, API, X, Y) %>%
    summarise(count = n(), raw_annual_volume = sum(Volume))
```


Let's see how many of the aggregate well rows have more than 12 records in their grouping:


```{r, warning = F, message = F}
head(filter(wells_2013, count > 12))
```


Some wells have 24 records in their count. Let's pick one of the `API` values at random:


```{r, warning = F, message = F}
filter(injections_2013, API == '35003220190000', Month == 'MAY') %>% select(Month, Volume)
```

Looks like a double count. But not all the overcounts are at 24 records:

```{r, warning = F, message = F}
filter(wells_2013, count > 12) %>%
  group_by(count) %>%
  summarise(num_of_wells = n()) %>%
  arrange(count)
```

_Sigh._ Rather than do a proper investigation and cleaning of this anomaly, I propose this (mis)use of math: for wells that have a `count` greater than 12, divide their `annual_total` by the ratio of `count/12`. For example, a well with a `count` of __24__ would have its `annual_total` divided by __2__, i.e. `24 / 2`. We'll store this value in a new column, `adjusted_annual_volume`:

```{r, warning = F, message = F}
wells_2013 <- wells_2013 %>%
  mutate(adjusted_annual_volume = ifelse(count <= 12, raw_annual_volume, raw_annual_volume / (count / 12)))

```

The result of that mutation:

```{r, warning = F, message = F}
wells_2013[wells_2013$API == '35003220190000', ]
```


Using the `adjusted_annual_volume`, How close are we to Walsh's estimate of 160 million barrels a month?


```{r, warning = F, message = F}
100 * (160000000 - (sum(wells_2013$adjusted_annual_volume) / 12)) / 160000000
```


About 10 percent off. Well, at least we're in the same ballpark...This discrepancy seems reasonable given that I've given about zero effort to data cleaning...

#### Spatial filtering

But there is one more thing we can do: There are typos in the `X` and `Y` columns, i.e. the wells aren't properly geolocated. We could hand-inspect each instance and try to manually fix the data ourselves. But nah, let's just use `ok_map` to filter out all wells that aren't within the Oklahoma state boundaries:


```{r, warning = F, message = F}
gdf <- as.data.frame(wells_2013) # for some reason, grouped dataframes don't work
sp_wells_2013 <-  SpatialPointsDataFrame(data = gdf,
                          coords = gdf[, c("X", "Y")])
sp_wells_2013@proj4string <- ok_map@proj4string
# create a temp dataframe of the sp_wells_2013 rows and STUSPS, i.e. 'OK'
xdf <- over(sp_wells_2013, ok_map[, 'STUSPS'])
wells_2013$is_OK <- xdf$STUSPS == 'OK'
```

_Now_ let's calculate the average injection volume per month and find how much we're off compared to Walsh's estimation of 160 million barrels a month:

```{r, warning = F, message = F}
100 * (160000000 - (sum(filter(wells_2013, is_OK)$adjusted_annual_volume) / 12)) / 160000000
```

Only 1.6% off. Not bad for doing the most blunt and shallow kind of data cleaning.

But how different is the volume of the mappable wells compared to the non-mappable wells in our dataset?


```{r, warning = F, message = F}
# TKTKTK
100 * (sum(wells_2013$adjusted_annual_volume) -
         sum(filter(wells_2013, is_OK)$adjusted_annual_volume)) /
  sum(wells_2013$adjusted_annual_volume)
```

By excluding the non-mappable wells, we lose more than 11% of the total `adjusted_annual_volume`. It's a little disconcerting that more than 11% of _possibly_ relevant data is being ignored because their coordinates are gibberish -- and that's only scratching the surface of possible data entry problems. But, if ~160M barrels/month is good enough for _Walsh et al_, I think we can move on. Again, as I cannot stress enough: this is only a walkthrough, not an actual  research paper.



### 2012 UIC data




```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
fname <- "./data/2012_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2012Injections20141029.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2012  <- read_excel(fname)
```

Among the new problems 2012 has:

```{r, warning = F, message = F}
injections_2012 <- xl_2012 %>%
    mutate(Volume = as.numeric(Volume),
          API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(ISDELETE == 0, SALTWATER == 1, STATUS == 'ACCEPTED',
           !is.na(Volume))
```


Let's do an aggregation and get the `raw_annual_volume`. Then derive `clean_wells` by compensating for wells in which the count of records is more than 12:


```{r, warning = F, message = F}
wells_2012 <-  injections_2012 %>%
    group_by(ReportYear, API) %>%
    summarise(count = n(), raw_annual_volume = sum(Volume))
```

Perform the cleaning/averaging for calculating `adjusted_annual_volume`:

```{r, warning = F, message = F}
wells_2012 <- wells_2012 %>%
  mutate(adjusted_annual_volume = ifelse(count <= 12, raw_annual_volume, raw_annual_volume / (count / 12)))
```


#### Left join for latitude/longitudes

So the big problem with the 2012 data is that there is no `X` or `Y` column, i.e. the geospatial data.

But we can fix that by joining `wells_2012` to `wells_2013`, the latter of which _does_ contain geospatial data. Both tables contain an `API` column. But let me be clear: it is a _major_ assumption to think that the API values, whatever they stand for, aren't also filled with typos or nonsense data.

For SQL fans, the dplyr package has a conveniently named __left_join()__ function. If you're fuzzy on how left joins work, remember that it preserves all the records on the "left" side -- in this case, `wells_2012`. For wells in `wells_2012` that don't have a corresponding match of `API` in `wells_2013`, the columns `X`, `Y`, and `is_OK` will be NA:


```{r, warning = F, message = F}
wells_2012 <- wells_2012 %>%
  left_join(select(as.data.frame(wells_2013), API, X, Y, is_OK), by = 'API') %>%
  mutate(is_OK = !is.na(is_OK))
```

Let's calculate the monthly average for 2012 for the mappable wells:

```{r, warning = F, message = F}
sum(filter(wells_2012, is_OK)$adjusted_annual_volume) / 12
```

And how much did we lose by removing wells in 2012 that didn't have a matching API in 2013?


```{r, warning = F, message = F}
100 * (sum(wells_2012$adjusted_annual_volume) -
         sum(filter(wells_2012, is_OK)$adjusted_annual_volume)) /
  sum(wells_2012$adjusted_annual_volume)
```

That's more than a rounding error within the 2012 data...but moving on to 2011.

### 2011 data

And the data keeps getting quirkier:

```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
fname <- "./data/2011_UIC_Injection_Volumes_Report.xlsx"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/2011INJECTIONVOLUMES.xlsx"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2011 <- read_excel(fname)
```

To see how ugly things are, check out the column names:

```{r, warning = F, message = F}
names(xl_2011)
```

Columns 27 to 49 exist, but have null values. Usually this is the result of some sloppy mainframe-to-spreadsheet program that someone wrote decades ago. But now we have to deal with it because R does not like it when a data frame has duplicate values in the column names.

This turns out not to be too horrible, using standard R syntax to subset the selection by choosing only the first 27 columns. Then we apply the same mutations and filters as we did for the 2012 data set. We have to make a few changes because whoever maintains the 2011 dataset decided to use a whole different naming scheme, e.g. `IsDelete` instead of `ISDELETE` _and_ datatype convention, e.g. `"0"` instead of `0`:

```{r, warning = F, message = F}
injections_2011 <- xl_2011[, c(1:26)] %>%
    mutate(Volume = as.numeric(Volume),
          API = as.character(API), ReportYear = as.numeric(ReportYear)) %>%
    filter(IsDelete == "0", SaltWater == "1", Status == 'ACCEPTED',
           !is.na(Volume))
```

Like 2012, there are no `X` and `Y`  columns. So we repeat 2012's filtering and joining process:

```{r, warning = F, message = F}
# sum the wells
wells_2011 <-  injections_2011 %>%
    group_by(ReportYear, API) %>%
    summarise(count = n(), raw_annual_volume = sum(Volume))
# clean the wells
wells_2011 <- wells_2011 %>%
  mutate(adjusted_annual_volume = ifelse(count <= 12, raw_annual_volume, raw_annual_volume / (count / 12)))
# geo-join the wells
wells_2011 <- wells_2011 %>%
  left_join(select(as.data.frame(wells_2013), API, X, Y, is_OK), by = 'API') %>%
  mutate(is_OK = !is.na(is_OK))
```

Again, let's calculate the percent difference between the mappable and non-mappable wells:

```{r, warning = F, message = F}
100 * (sum(wells_2011$adjusted_annual_volume) -
         sum(filter(wells_2011, is_OK)$adjusted_annual_volume)) /
  sum(wells_2011$adjusted_annual_volume)
```

It's roughly the same difference as found in 2012. Meh, moving on.


### 2006 to 2010 UIC data

The next data file combines the years 2006 to 2010 into a single spreadsheet. That's a strong cue that the data structure will likely be signficantly different than what we have so far:

```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
fname <- "./data/all_2006-2010uic_1012a.xls"
if (!file.exists(fname)){
  url <- "http://www.occeweb.com/og/all_2006-2010uic_1012a.xls"
  print(paste("Downloading: ", url))
  download.file(url, fname)
}
xl_2006_2010 <- read_excel(fname)
```

You can run `names(xl_2006_2010)` yourself, but the upshot is that this data file is much simpler than what we have so far with only 10 columns. Each row represents an annual measurement of volume, which saves us the time of aggregating it ourselves.

Unlike the other datasets, there is a `FLUID_TYPE` column which very _unhelpfully_ contains values such as `"S"` and `""`:


```{r, warning = F, message = F}
group_by(xl_2006_2010, FLUID_TYPE) %>% summarize(count = n())
```

Since saltwater injections make up the vast majority of UIC wells, I'm just going to assume that `S` stands for "saltwater". As a bonus nusiance, the column name for `"TOTAL VOLUME"` is a bit jacked up with extra whitespace. So I'm just going to manually rename the columns as part of the initial data filtering step:

```{r, warning = F, message = F}
# rename the columns
names(xl_2006_2010) <- c('API_COUNTY', 'API', 'LEASE_NAME', 'WELL_NUMBER', 'Y', 'X', 'ReportYear', 'FLUID_TYPE', 'PACKERDPTH', 'sub_raw_annual_volume')
```

Since this data is already aggregated by year, we _could_ skil the summarization step. But -- surprise, surprise -- it turns out there are multiple annual saltwater injection records per well. And it doesn't appear that the records are just duplicates. So, rather than calculate an average, I set `adjusted_annual_volume` to be the biggest (i.e. `max()`) value of the duplicate records:

```{r, warning = F, message = F}
# filter the data
wells_2006_2010 <- xl_2006_2010 %>%
    mutate(sub_raw_annual_volume = as.numeric(sub_raw_annual_volume),
           API = paste(as.character(API_COUNTY), as.character(API), sep = ''), ReportYear = as.numeric(ReportYear)) %>%
    group_by(API, X, Y, ReportYear) %>%
    summarise(count = n(), raw_annual_volume = sum(sub_raw_annual_volume), adjusted_annual_volume = max(sub_raw_annual_volume))
```




Since the 2006-2010 data comes with `X` and `Y`, we don't need to do a table join with `wells_2013`. But we do need to repeat the spatial filtering with `ok_map`:


```{r, warning = F, message = F}
gdf <- as.data.frame(wells_2006_2010)
sp_wells_2006_2010 <-  SpatialPointsDataFrame(data = gdf,
                          coords = gdf[, c("X", "Y")])
sp_wells_2006_2010@proj4string <- ok_map@proj4string
# create a temp dataframe of the sp_wells_2013 rows and STUSPS, i.e. 'OK'
xdf <- over(sp_wells_2006_2010, ok_map[, 'STUSPS'])
wells_2006_2010$is_OK <- xdf$STUSPS == 'OK'
```

Let's calculate the difference between mapped and non-mappable wells:

```{r, warning = F, message = F}
100 * (sum(wells_2006_2010$adjusted_annual_volume) -
         sum(filter(wells_2006_2010, is_OK)$adjusted_annual_volume)) /
  sum(wells_2006_2010$adjusted_annual_volume)
```

Less than a single percent. That's nice. Though that doesn't help us with the discrepencies in years 2011, 2012, and 2013...


### All the UIC data together now

We can now create a data frame of annual saltwater injection volumes for 2006 to 2013. All that grunt work makes this collation quite easy. I use dplyr's __bind_rows()__ function, which I believe is just a wrapper around __rbind()__. Some of the wells have typos in the `ReportYear`, so we apply a filter to remove all years greater than 2013:

```{r, warning = F, message = F}
all_wells <- bind_rows(wells_2013,
                              wells_2012,
                              wells_2011,
                              wells_2006_2010) %>%
                    filter(ReportYear <= 2013)
```


Now let's make a histogram:


```{r, warning = F, message = F}
ggplot(data = group_by(all_wells, ReportYear)
                          %>% summarize(raw_annual_volume = sum(raw_annual_volume)),
        aes(x = ReportYear, y = raw_annual_volume)) +
  geom_bar(stat = 'identity') +
  ggtitle("Mappable wells")
```



Compare the two:


```{r, warning = F, message = F}
ggplot() +

  # mappable + non-mappable, non-adjusted volume
  geom_bar(
           data = all_wells %>%
                    group_by(ReportYear) %>%
                    summarize(total_volume = sum(raw_annual_volume)),
           aes(x = ReportYear, y = total_volume / 1000000),
           stat = 'identity', fill = "firebrick") +

  # mappable + non-mappable, adjusted volume
  geom_bar(
           data = all_wells %>%
                    group_by(ReportYear) %>%
                    summarize(total_volume = sum(adjusted_annual_volume)),
           aes(x = ReportYear, y = total_volume / 1000000),
           stat = 'identity', fill = "darkorange") +

  # mappable, adjusted volume
  geom_bar(
           data = filter(all_wells, is_OK) %>%
                  group_by(ReportYear) %>%
                  summarize(total_volume = sum(adjusted_annual_volume)),
           aes(x = ReportYear, y = total_volume / 1000000),
           stat = 'identity', fill = "gray") +

    scale_y_continuous(labels = comma, expand = c(0, 0)) +
    annotate("text", x = 2006, y = 2450, family = dan_base_font(), size = 4,
             label = "Raw volume", color = "firebrick") +
    annotate("text", x = 2006, y = 2300, family = dan_base_font(), size = 4,
             label = "Adjusted volume", color = "darkorange") +
    annotate("text", x = 2006, y = 2150, family = dan_base_font(), size = 4,
             label = "Adjusted volume that is mappable", color = "#777777") +


    theme(axis.title = element_text(color = 'black')) +
    xlab("") + ylab("Barrels injected (millions)") +
    ggtitle("Difference in annual saltwater injection volumes between:\nraw values, adjusted values, and mappable adjusted values")



```

TK explanation



Summarise by location


```{r, warning = F, message = F}
q1 <- ggplot(data = all_wells %>%
         group_by(Y, ReportYear) %>% summarize(total = sum(raw_annual_volume)),
       aes(x = factor(ReportYear), y = total, order = desc(Y >= 36),
           fill = Y >= 36)) + geom_bar(stat = 'identity') +
        scale_fill_manual(values = c("#EEEEEE", "#FF6600"), labels = c("Below 36.5", "Above 36.5")) +
        scale_x_discrete(limits = c(2006:2015), labels = c("2012", "2013") )+
      guides(fill = guide_legend(reverse = F))
```


Histogram of quakes

```{r, warning = F, message = F}
q2 <- ggplot(data = filter(ok_quakes, year >= 2006),
       aes(x = factor(year), order = desc(latitude >= 36),
           fill = latitude >= 36)) + geom_histogram(binwidth = 1) +
        scale_fill_manual(values = c("#EEEEEE", "#FF6600"), labels = c("Below 36.5", "Above 36.5")) +
      guides(fill = guide_legend(reverse = F))
```


```{r, warning = F, message = F}
grid.arrange(
  q1 + theme(axis.text.y = element_blank()) + ggtitle("Annual injection volumes"),
  q2 + theme(axis.text.y = element_blank()) + ggtitle("Annual count of M3.0+ earthquakes")
)
```



Facet wrap the wells:

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  geom_point(data = filter(all_wells, raw_annual_volume <= 360000),
             aes(x = X, y = Y),
             alpha = 0.3, shape = 1, color = 'yellow') +
  geom_point(data = filter(all_wells, raw_annual_volume >= 360000),
             aes(x = X, y = Y),
             alpha = 0.3, shape = 1, color = 'purple') +
  # geom_point(data = filter(ok_quakes, year >= 2012), aes(x = longitude, y = latitude),
  #           alpha = 0.1, shape = 1, color = 'red') +
  coord_map("albers", lat0 = 38, latl = 42) + theme_dan_map() +
  ggtitle("2013 UIC and quakes")  +
  facet_wrap(~ ReportYear, ncol = 2)
```

TODO: Hexbin them


Counts:

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +

  stat_binhex(data = all_wells, aes(x = X, y = Y, alpha = ..count..), bins = 30 ) +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```


Volume



```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = all_wells,
                   aes(x = X, y = Y, z = raw_annual_volume),
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 1)
```



Hi-lo pass:

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = all_wells,
                   aes(x = X, y = Y, z = raw_annual_volume),
                   bins = 30,
                   fun = function(z){
                     sz <- min(sum(z), 60000000)
                     ifelse(sz > 10000000, sz, NA)
                    },
                   color = 'white', drop = TRUE ) +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```





Do side by side facet of earthquakes


```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_binhex(data = filter(ok_quakes, year >= 2012, year <= 2014),
              aes(x = longitude, y = latitude), bins = 30, color = "#FFFFFF") +
  scale_fill_gradient(low = "#ECDDDD", high = "red") +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 1)
```



#### Draw a highlight box

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear >= 2010),
                   aes(x = X, y = Y, z = raw_annual_volume),
                   bins = 30,
                   fun = function(z){
                     sz <- min(sum(z), 60000000)
                     ifelse(sz > 10000000, sz, NA)
                    },
                   color = 'white', drop = TRUE ) +
  geom_rect(data = ok_map, aes(
      xmin = -99, xmax = -97.5,
      ymin = 36, ymax = 37
    ), fill = "NA", color = "red", linetype = "dashed", size = 0.5) +
  scale_fill_gradientn(colours = c("#DDDDEC", "purple"), na.value = NA) +

  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ ReportYear, ncol = 2)
```

Do the same with earthquakes:

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_binhex(data = filter(ok_quakes, year >= 2011, year <= 2014),
              aes(x = longitude, y = latitude), bins = 30, color = "#FFFFFF") +
  geom_rect(data = ok_map, aes(
      xmin = -99, xmax = -97.5,
      ymin = 36, ymax = 37
    ), fill = "NA", color = "red", linetype = "dashed", size = 0.5) +

  scale_fill_gradient(low = "#ECDDDD", high = "red") +
  coord_equal() +
  theme_dan_map() +
  facet_wrap(~ year, ncol = 2)
```


### Test, volume and alpha:

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear == 2006),
                   aes(x = X, y = Y, z = raw_annual_volume),
                   alpha = 0.5,
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "#002BFF"), na.value = NA) +
    coord_equal() +
  theme_dan_map()
```

```{r, warning = F, message = F}
ggplot() +
  geom_polygon(data = ok_map, aes(x = long, y = lat, group = group),
               fill = "white", color = "black") +
  stat_summary_hex(data = filter(all_wells, ReportYear == 2013),
                   aes(x = X, y = Y, z = raw_annual_volume),
                   alpha = 0.5,
                   bins = 30,
                   fun = function(z){ sum(z) },
                   color = 'white') +
  scale_fill_gradientn(colours = c("#DDDDEC", "#FFD500"), na.value = NA) +
    coord_equal() +
  theme_dan_map()
```



# Gas prices

https://research.stlouisfed.org/fred2/series/DCOILWTICO/downloaddata


```{r, warning = F, message = F}
fname <- './data/stlouisfed-oil-prices.csv'
if (!file.exists(fname)){
  curlstr <- paste("curl -s --data 'form%5Bnative_frequency%5D=Daily&form%5Bunits%5D=lin&form%5Bfrequency%5D=Daily&form%5Baggregation%5D=Average&form%5Bobs_start_date%5D=1986-01-02&form%5Bobs_end_date%5D=2015-08-24&form%5Bfile_format%5D=csv&form%5Bdownload_data_2%5D='", '-o', fname)
  print(paste("Downloading: ", fname))
  system(curlstr)
}
gasdata <- read.csv("./data/stlouisfed-oil-prices.csv", stringsAsFactors = F) %>%
  filter(year(DATE) >= 2008, VALUE != '.')  %>%
  mutate(week = floor_date(as.Date(DATE), 'week'), VALUE = as.numeric(VALUE), panel = "GAS") %>%
  group_by(week) %>% summarize(avg_value = mean(VALUE))


```







```{r, warning = F, message = F}
okquakes_weekly <- mutate(ok_quakes, week = floor_date(date, 'week'), panel = 'Quakes') %>%
  filter(year >= 2008) %>%
  group_by(week) %>%
  summarize(count = n())

gp <- ggplot() + scale_x_date(breaks = pretty_breaks(),
                              limits = c(as.Date('2008-01-01'), as.Date('2015-08-31')))

quakesg <- gp + geom_bar(data = okquakes_weekly, aes(x = week, y = count), stat = "identity", position = 'identity', color = 'black') +
  scale_y_continuous(expand = c(0, 0)) +
  ggtitle("Weekly Counts of M3.0+ Earthquakes in Oklahoma")

gasg <- gp + geom_line(data = gasdata, aes(x = week, y = avg_value), size = 0.5, color = 'black') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 150)) +
  ggtitle("Weekly Average Oil Prices, Dollars per Barrel")
```


```{r, warning = F, message = F}
# http://stackoverflow.com/questions/10197738/add-a-footnote-citation-outside-of-plot-area-in-r

annotations <- list(
  list(date = "2011-11-05", label = "5.6M earthquake hits Oklahoma", letter = 'A'),
  list(date = "2014-02-18", label = 'OGS states that "Overall, the majority, but not all, of the recent earthquakes appear\nto be the result of natural stresses."', letter = 'B'),
  list(date = "2015-04-21", label = 'OGS reverses its opinion: "The rates and trends in seismicity are very unlikely\nto represent a naturally occurring process"', letter = 'C')
)


datelines <- lapply(annotations, function(a){
  wk <- as.numeric(floor_date(as.Date(a$date), 'week'))
  geom_vline(xintercept = wk, color = "red",
             linetype = 'dashed')
})

datelabels <- lapply(annotations, function(a){
  dt <- as.Date(a$date)
  annotate("text", x = dt, y = Inf, size = rel(4.0),  label = a$letter, vjust = 1.0, fontface = 'bold', color = "red", hjust = 1.5)
})

ttheme <- theme(plot.title = element_text(size = 12))

footnotes = paste(lapply(annotations, function(a){
              paste(a$letter,
              paste(strftime(a$date, '%b %d, %Y'), a$label, sep = ' - '),
              sep = '. ')
    }),
  collapse = '\n')

p <- arrangeGrob(quakesg + datelines + datelabels + ttheme,
                 gasg + datelines + datelabels + ttheme,
                 bottom = textGrob(label = footnotes, x = unit(1, 'cm'),
                                         hjust = 0.0, vjust = 0.3,
                                         gp = gpar(fontsize = rel(10), fontfamily = "Gill Sans MT"))
                 )

plot.new()
grid.draw(p)
```




The study, titled "Oklahoma's Recent Earthquakes and Saltwater Disposal," was funded by the Stanford Center for Induced and Triggered Seismicity, an affiliate program at the School of Earth, Energy & Environmental Sciences.


> Active faults in Oklahoma might trigger an earthquake every few thousand years. However, by increasing the fluid pressure through disposal of wastewater into the Arbuckle formation in the three areas of concentrated seismicity – from about 20 million barrels per year in 1997 to about 400 million barrels per year in 2013 – humans have sped up this process dramatically. "The earthquakes in Oklahoma would have happened eventually," Walsh said. "But by injecting water into the faults and pressurizing them, we've advanced the clock and made them occur today."



http://news.stanford.edu/news/2015/june/okla-quake-drilling-061815.html






## Trying Google Maps


```{r, warning = F, message = F, cache = TRUE, cache.path = "/tmp/rstudio-cache/ok-earthquakes-4"}
library(ggmap)
ok_goog_map <- get_googlemap(center = c(lon = -99.007, lat = 35.38905),  size = c(450,250), zoom = 6, scale = 2, maptype = 'terrain',
style = c(feature = "administrative.province", element = "labels", visibility = "off"))
ggmap(ok_goog_map)
```



------




## Things TODO

- Use USGS Geologic map data: https://mrdata.usgs.gov/geology/state/state.php?state=OK



Reference to disposal wells in OK gov:

http://earthquakes.ok.gov/what-we-know/earthquake-map/


On April 21, earthquakes.ok.gov was launched:

http://earthquakes.ok.gov/news/



## The state of research

[NPR's Oklahoma StateImpact project has a nice reading list](https://stateimpact.npr.org/oklahoma/2015/05/05/stateimpacts-earthquake-research-reading-list/), and so does the state of Oklahoma on [earthquakes.ok.gov](http://earthquakes.ok.gov/what-we-know/academic-research/). Much of what I include below is cribbed from those collections:


-  [Texas Railroad Commission Refutes Study Linking Quakes to Oil and Gas Industry](https://stateimpact.npr.org/texas/2015/09/02/texas-railroad-commission-refutes-study-linking-quakes-to-oil-and-gas-industry/)



http://www.newyorker.com/magazine/2015/04/13/weather-underground


> Holland had been clear about the connections between disposal wells and earthquakes, and during the socializing a researcher from Princeton observed that Holland’s position seemed to have shifted from that represented in O.G.S. statements. “Let me think how I can answer that while there’s a reporter standing right there,” Holland said, lightly. “The O.G.S. is a nonacademic department of a state university that, like many state universities, doesn’t get that much funding from the state.” The O.G.S. is part of O.U.’s Mewbourne College of Earth and Energy, which also includes the ConocoPhillips School of Geology and Geophysics. About seventeen per cent of O.U.’s budget comes from the state. “I prepare twenty pages for those statements and what comes out is one page. Those are not necessarily my words.”


In Science Magazine ("[High-rate injection is associated with the increase in U.S. mid-continent seismicity](https://profile.usgs.gov/myscience/upload_folder/ci2015Jun1814143055600Weingarten_etal.pdf)"), USGS scientists assert that "the entire increase in earthquake rate is associated with fluid injection wells."


[High-rate injection is associated with the increase in U.S. mid-continent seismicity
](http://www.sciencemag.org/content/348/6241/1336)

> Wastewater injection wells induce earthquakes that garner much attention, especially in tectonically inactive regions. Weingarten et al. combined information from public injection-well databases from the eastern and central United States with the best earthquake catalog available over the past 30 years. The rate of fluid injection into a well appeared to be the most likely decisive triggering factor in regions prone to induced earthquakes. Along these lines, Walsh III and Zoback found a clear correlation between areas in Oklahoma where waste saltwater is being injected on a large scale and areas experiencing increased earthquake activity.


[Oklahoma’s recent earthquakes and saltwater disposal](http://advances.sciencemag.org/content/1/5/e1500195.article-info) - F. Rall Walsh III and Mark D. Zoback

> Over the past 5 years, parts of Oklahoma have experienced marked increases in the number of small- to moderate-sized earthquakes. In three study areas that encompass the vast majority of the recent seismicity, we show that the increases in seismicity follow 5- to 10-fold increases in the rates of saltwater disposal. Adjacent areas where there has been relatively little saltwater disposal have had comparatively few recent earthquakes. In the areas of seismic activity, the saltwater disposal principally comes from “produced” water, saline pore water that is coproduced with oil and then injected into deeper sedimentary formations. These formations appear to be in hydraulic communication with potentially active faults in crystalline basement, where nearly all the earthquakes are occurring. Although most of the recent earthquakes have posed little danger to the public, the possibility of triggering damaging earthquakes on potentially active basement faults cannot be discounted.

TK img: http://d3a5ak6v9sb99l.cloudfront.net/content/advances/1/5/e1500195/F1.large.jpg?download=true


TK img: http://d3a5ak6v9sb99l.cloudfront.net/content/advances/1/5/e1500195/F2.large.jpg?width=800&height=600&carousel=1


- [Sharp increase in central Oklahoma seismicity since 2008 induced by massive wastewater injection](http://www.sciencemag.org/content/345/6195/448)



> Unconventional oil and gas production provides a rapidly growing energy source; however, high-production states in the United States, such as Oklahoma, face sharply rising numbers of earthquakes. Subsurface pressure data required to unequivocally link earthquakes to wastewater injection are rarely accessible. Here we use seismicity and hydrogeological models to show that fluid migration from high-rate disposal wells in Oklahoma is potentially responsible for the largest swarm. Earthquake hypocenters occur within disposal formations and upper basement, between 2- and 5-kilometer depth. The modeled fluid pressure perturbation propagates throughout the same depth range and tracks earthquakes to distances of 35 kilometers, with a triggering threshold of ~0.07 megapascals. Although thousands of disposal wells operate aseismically, four of the highest-rate wells are capable of inducing 20% of 2008 to 2013 central U.S. seismicity.


- [Myths and Facts on Wastewater Injection, Hydraulic Fracturing, Enhanced Oil Recovery, and Induced Seismicity](http://srl.geoscienceworld.org/content/86/4/1060.full)


- [Observations of static Coulomb stress triggering of the November 2011 M5.7 Oklahoma earthquake sequence](http://onlinelibrary.wiley.com/doi/10.1002/2013JB010612/abstract)


TKTK:
http://energyindepth.org/national/five-things-to-know-about-a-new-stanford-oklahoma-earthquake-study/
